# airflow user credentials
_AIRFLOW_WWW_USER_USERNAME='airflow'
_AIRFLOW_WWW_USER_PASSWORD='senhaairflow'

# airflow setup
AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION='true'
AIRFLOW__CORE__LOAD_EXAMPLES='false'
AIRFLOW__API__AUTH_BACKENDS='airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK='true'

AIRFLOW__CORE__SQL_ALCHEMY_CONN='postgresql+psycopg2://airflow:airflow@postgres/airflow'
AIRFLOW__CORE__FERNET_KEY='chavefernet'
AIRFLOW__CORE__EXECUTOR='LocalExecutor'
AIRFLOW__CORE__PARALLELISM=2
AIRFLOW__CORE__DAG_CONCURRENCY=4

# airflow connections defined as environment variables
# https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/connections/aws.html
AIRFLOW_CONN_AWS_POLITICALDATALAKE='aws://awskeyid:awssecretkey@/?region_name=sa-east-1'

# deactivate exposure warning
AIRFLOW__WEBSERVER__WARN_DEPLOYMENT_EXPOSURE='False'

# setup logging to remote
# AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID='aws_default'
# AIRFLOW__LOGGING__REMOTE_LOGGING='True'
# AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER='s3://mybucket'

AIRFLOW_VAR_EMR_SERVERLESS_APPLICATION_ID='myemapplicationid'
AIRFLOW_VAR_EMR_SERVERLESS_EXECUTION_ROLE_ARN='myemrrole'
AIRFLOW_VAR_S3_SCRIPTS_BUCKET='myscriptsbucket'
